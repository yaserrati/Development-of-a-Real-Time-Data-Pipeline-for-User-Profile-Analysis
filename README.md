  # Development-of-a-Real-Time-Data-Pipeline-for-User-Profile-Analysis
This training program aims to empower participants to design, develop and deploy a real-time data pipeline using PySpark, Kafka, Cassandra and MongoDB. Participants will learn how to efficiently transform, aggregate, and store user data generated by randomuser.me.



# Planification de project:
<img width="854" alt="image" src="https://github.com/yaserrati/Development-of-a-Real-Time-Data-Pipeline-for-User-Profile-Analysis/assets/88887542/eb96beaf-5e60-40eb-8b4e-f2de5099209a">

# Introduction:
Introduction :

Dans un monde où les données sont considérées comme le nouvel or, il est impératif pour les organisations de pouvoir traiter et analyser les données en temps réel pour prendre des décisions éclairées. Ce programme est conçu pour les professionnels de la donnée qui cherchent à acquérir des compétences pratiques dans la mise en œuvre de pipelines de données en temps réel.

Notre projet consiste à mettre en place un pipeline de données en temps réel afin de relever les défis liés à la gestion et à l'analyse de données en temps réel. Nous utiliserons des technologies telles que PySpark, Kafka, Cassandra et MongoDB pour collecter, transformer et stocker les données de manière efficace et sécurisée.

L'objectif de ce rapport est de présenter les différentes étapes du projet et de décrire en détail les solutions mises en œuvre pour chaque étape. Nous commencerons par la configuration de l'environnement en installant les dépendances nécessaires et en mettant en place les outils tels que PySpark, Kafka, Cassandra et MongoDB. Ensuite, nous détaillerons la mise en place des fonctions de sauvegarde des données dans Cassandra et MongoDB.

Une fois l'environnement configuré, nous créerons une session Spark qui nous permettra de traiter les données en temps réel de manière efficace. Nous lirons ensuite les données en streaming à partir du topic Kafka spécifié, en les transformant selon nos besoins. Nous effectuerons des opérations de transformation telles que la construction du nom complet, la validation ou le recalcul de l'âge, et la construction d'une adresse complète.

Les données transformées seront ensuite sauvegardées dans une table Cassandra pour une analyse ultérieure. Nous mettrons également en place des agrégations pour extraire des insights, tels que le nombre d'utilisateurs par nationalité, l'âge moyen des utilisateurs et les domaines de courriel les plus courants. Les résultats agrégés seront stockés dans des collections MongoDB.

Nous surveillerons la sortie de la console pour vérifier les résultats agrégés et détecter d'éventuelles erreurs. Enfin, nous créerons des tableaux de bord interactifs à l'aide de Python Dash pour visualiser les données agrégées au niveau de MongoDB.

Ce rapport détaillera également les mesures de sécurité mises en place pour assurer la conformité avec le RGPD. Nous rédigerons un registre détaillant tous les traitements de données personnelles effectués, les types de données stockées, les finalités du traitement, et les mesures de sécurité mises en place. De plus, nous mettrons en œuvre des procédures de tri et de suppression des données personnelles inutiles ou trop anciennes, conformément aux exigences du RGPD.

En conclusion, la mise en place de ce pipeline de données en temps réel nous permettra de collecter, transformer, stocker et analyser les données de manière efficace et sécurisée. Ce rapport détaillera les différentes étapes du projet ainsi que les solutions mises en œuvre pour relever les défis liés à la gestion des données en temps réel.





# Objectifs du projet
# Configuration de l'environnement
# Collecte des données en temps réel avec Kafka
# Transformation et agrégation des données
# Stockage des données dans Cassandra
# Stockage des résultats agrégés dans MongoDB
# Surveillance et validation des résultats
# Visualisation des données à l'aide de tableaux de bord interactifs


# Mesures de sécurité et conformité avec le RGPD
Dans le cadre de la conformité avec le RGPD, des mesures de sécurité ont été mises en place pour garantir la confidentialité et la protection des données sensibles. Parmi ces mesures, nous avons appliqué un processus de chiffrement sur certaines colonnes sensibles telles que l'email, le numéro de téléphone et l'adresse.

Pour réaliser le chiffrement, nous avons utilisé l'algorithme de hachage SHA-256. Cet algorithme transforme les données en une empreinte numérique unique et irréversible. Ainsi, même en cas de compromission des données, il serait extrêmement difficile de retrouver les informations d'origine.

Dans notre pipeline de données, nous avons utilisé la fonction sha2 de PySpark pour appliquer le chiffrement sur les colonnes concernées. Par exemple, le code suivant illustre le chiffrement des colonnes "email", "phone" et "address" :
````
#  encrypt  email, password,phone, and cell using SHA-256
result_df = result_df.withColumn("email", F.sha2(result_df["email"], 256))
result_df = result_df.withColumn("phone", F.sha2(result_df["phone"], 256))
result_df = result_df.withColumn("address", F.sha2(result_df["address"], 256))
````
Ce processus de chiffrement renforce la protection des données sensibles en rendant les informations d'identification personnelles pratiquement indéchiffrables sans la clé correspondante. Ainsi, même en cas d'accès non autorisé aux données, les informations personnelles restent sécurisées.

En appliquant ce chiffrement sur les colonnes sensibles, nous avons respecté les principes fondamentaux du RGPD en garantissant la confidentialité et la protection des données personnelles. Les données chiffrées peuvent être utilisées en toute sécurité pour des analyses ultérieures sans compromettre la vie privée des individus concernés.

Il convient de noter que la sécurité et la protection des données ne se limitent pas au chiffrement. D'autres mesures de sécurité, telles que la gestion des accès et les politiques de confidentialité, doivent également être mises en place pour garantir une conformité complète avec les exigences du RGPD.

# Conclusion :

En conclusion, la mise en place de ce pipeline de données en temps réel a permis de collecter, transformer, stocker et analyser les données de manière efficace et sécurisée. Grâce à l'utilisation de technologies telles que PySpark, Kafka, Cassandra et MongoDB, les organisations ont pu prendre des décisions éclairées en temps réel.

Le pipeline a utilisé Kafka pour collecter les données en streaming de manière fiable et évolutive. Les données ont ensuite été transformées et agrégées pour extraire des insights pertinents. Les résultats ont été stockés dans Cassandra pour une analyse ultérieure, tandis que les agrégations étaient stockées dans MongoDB pour une visualisation interactive.

Des mesures de sécurité ont été mises en place pour garantir la conformité avec le RGPD. Un registre des traitements de données personnelles a été établi et des procédures de tri et de suppression des données inutiles ont été appliquées.

En résumé, ce projet a permis aux organisations de tirer parti des données en temps réel pour prendre des décisions éclairées. Le pipeline a fourni des fonctionnalités de collecte, de transformation, de stockage et d'analyse des données en temps réel, ouvrant ainsi de nouvelles opportunités dans le domaine de la gestion des données en temps réel.
